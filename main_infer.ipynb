{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ostix360/quiet-star/blob/565852d6f105cc117d006740af9da1d8fe50ede6/quiet-star-infer.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.mistral import configuration_mistral as original_configuration_mistral\n",
    "from transformers.models.mistral import modeling_mistral as original_modeling_mistral\n",
    "\n",
    "import configuration_mistral\n",
    "import modeling_mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf500bfdc58e421c97db679f1be411d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_modeling_mistral.MistralModel = modeling_mistral.MistralModel\n",
    "original_modeling_mistral.MistralForCausalLM = modeling_mistral.MistralForCausalLM\n",
    "original_configuration_mistral.MistralConfig = configuration_mistral.MistralConfig\n",
    "\n",
    "model_path = \"ezelikman/quietstar-8-ahead\"\n",
    "\n",
    "n_ahead = 8\n",
    "n_ahead_talk = 1\n",
    "merged_talk_heads = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             load_in_8bit=True,\n",
    "                                             max_thoughts=n_ahead + n_ahead_talk + 1,\n",
    "                                             merged_talk_heads=merged_talk_heads,\n",
    "                                             merged_lm_and_talk_heads=False,\n",
    "                                             merged_lm_and_think_heads=True,\n",
    "                                             use_concat_talk_head=True,\n",
    "                                             use_shallow_think=True,\n",
    "                                             use_shallow_talk=False,\n",
    "                                             use_complex_think_head=False,\n",
    "                                             use_complex_talk_head=True,\n",
    "                                             use_weighted_talk_head=True,\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model.use_end_thought_token = True\n",
    "model.tokenizer = tokenizer\n",
    "model.use_start_thought_token = True\n",
    "model.wandb_enabled = True\n",
    "model.n_ahead = n_ahead\n",
    "model.n_passes = 1\n",
    "model.eval_mode = True\n",
    "model.first_run = False\n",
    "model.kill_after = 100\n",
    "model.rm_initialized = True\n",
    "\n",
    "model.original_mode = False\n",
    "\n",
    "input = \"Solve the equation 2x + 3w² = 5.\"\n",
    "\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# output = model.generate(input_ids, max_length=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(input_ids, attention_mask, model, temp, max_length=20):\n",
    "    with torch.no_grad():\n",
    "        finished_generating = torch.zeros(len(input_ids), dtype=torch.bool, device=input_ids.device)\n",
    "        for cur_token_idx in range(max_length):\n",
    "            # Sample the next token\n",
    "            new_ids = model(\n",
    "                input_ids[~finished_generating],\n",
    "                attention_mask=attention_mask[~finished_generating]\n",
    "            )['logits']\n",
    "            # Mask out the start and end thought tokens so we don't accidentally sample them\n",
    "            new_ids[:, :, model.tokenizer.vocab_size:] = -float(\"inf\")\n",
    "            for list_idx, answer_idx in enumerate((~finished_generating).nonzero(as_tuple=True)[0]):\n",
    "                # Find the index of the last token that is not padding\n",
    "                base_answer_ids = input_ids[answer_idx]\n",
    "                new_answer_ids = new_ids[list_idx]\n",
    "                last_token_idx = (base_answer_ids != model.tokenizer.pad_token_id).nonzero(as_tuple=True)[0].max()\n",
    "\n",
    "\n",
    "                new_ids_sampled = torch.multinomial(\n",
    "                        torch.nn.functional.softmax(new_answer_ids[last_token_idx] / temp, dim=-1), 1)\n",
    "                # Assign the new id to the last token\n",
    "                if last_token_idx + 1 >= len(base_answer_ids):\n",
    "                    # Add padding everywhere\n",
    "                    new_padding = torch.full((len(input_ids), 1), model.tokenizer.pad_token_id, dtype=torch.long,\n",
    "                                             device=input_ids.device)\n",
    "                    input_ids = torch.cat([input_ids, new_padding], dim=-1)\n",
    "                    attention_mask = torch.cat([attention_mask, torch.zeros_like(new_padding)], dim=-1)\n",
    "                attention_mask[answer_idx, last_token_idx + 1] = 1\n",
    "                input_ids[answer_idx, last_token_idx + 1] = new_ids_sampled\n",
    "                if new_ids_sampled == model.tokenizer.eos_token_id or new_ids_sampled == model.tokenizer.bos_token_id or new_ids_sampled == model.tokenizer.pad_token_id:\n",
    "                    finished_generating[answer_idx] = 1\n",
    "            if finished_generating.all():\n",
    "                break\n",
    "    return input_ids, attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Solve the equation 2x + 3w² = 5.\n",
      "\n",
      "#### Solution\n",
      "\n",
      "2x + 3w² = 5\n",
      "\n",
      "2x - 5 = - 3w²\n",
      "\n",
      "2x - 5 = - (3w) (w)\n",
      "\n",
      "2x - 5 = - 3w²\n",
      "\n",
      "2x - 3w² = - 5\n",
      "\n",
      "2x - 3w² + 3w² = - 5 + 3w²\n",
      "\n",
      "2x - \n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = generate(input_ids, torch.ones_like(input_ids), model, 0.9, max_length=100)\n",
    "\n",
    "\n",
    "print(tokenizer.decode(out[0][0], skip_special_tokens=False))\n",
    "\n",
    "print(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
